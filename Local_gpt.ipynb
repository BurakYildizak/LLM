{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32afa90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import click\n",
    "import torch\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler  # for streaming response\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.vectorstores import Chroma\n",
    "from huggingface_hub import hf_hub_download\n",
    "#from prompt_template_utils import get_prompt_template\n",
    "from transformers import (\n",
    "    GenerationConfig,\n",
    "    pipeline,\n",
    ")\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "MaxNewTokens=4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f45d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a helpful assistant, you will use the provided context to answer user questions.\n",
    "Read the given context before answering questions and think step by step. If you can not answer a user question based on\n",
    "the provided context, inform the user. Do not use any other information for answering user. Provide a detailed answer to the question.\"\"\"\n",
    "\n",
    "def get_prompt_template(system_prompt=system_prompt, promptTemplate_type=None, history=False):\n",
    "    if promptTemplate_type == \"llama\":\n",
    "        B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "        B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "        SYSTEM_PROMPT = B_SYS + system_prompt + E_SYS\n",
    "        if history:\n",
    "            instruction = \"\"\"\n",
    "            Context: {history} \\n {context}\n",
    "            User: {question}\"\"\"\n",
    "\n",
    "            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "            prompt = PromptTemplate(input_variables=[\"history\", \"context\", \"question\"], template=prompt_template)\n",
    "        else:\n",
    "            instruction = \"\"\"\n",
    "            Context: {context}\n",
    "            User: {question}\"\"\"\n",
    "\n",
    "            prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "            prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
    "    else:\n",
    "        # change this based on the model you have selected.\n",
    "        if history:\n",
    "            prompt_template = (\n",
    "                system_prompt\n",
    "                + \"\"\"\n",
    "\n",
    "            Context: {history} \\n {context}\n",
    "            User: {question}\n",
    "            Answer:\"\"\"\n",
    "            )\n",
    "            prompt = PromptTemplate(input_variables=[\"history\", \"context\", \"question\"], template=prompt_template)\n",
    "        else:\n",
    "            prompt_template = (\n",
    "                system_prompt\n",
    "                + \"\"\"\n",
    "\n",
    "            Context: {context}\n",
    "            User: {question}\n",
    "            Answer:\"\"\"\n",
    "            )\n",
    "            prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
    "\n",
    "    memory = ConversationBufferMemory(input_key=\"question\", memory_key=\"history\")\n",
    "\n",
    "    return (\n",
    "        prompt,\n",
    "        memory,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130f8012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_quantized_model_gguf_ggml(model_id, model_basename, device_type, logging):\n",
    "    path=\"./models\"\n",
    "\n",
    "    try:\n",
    "        logging.info(\"Using Llamacpp for GGUF/GGML quantized models\")\n",
    "        model_path = hf_hub_download(\n",
    "            repo_id=model_id,\n",
    "            filename=model_basename,\n",
    "            resume_download=True,\n",
    "            cache_dir=path,\n",
    "        )\n",
    "        kwargs = {\n",
    "            \"model_path\": model_path,\n",
    "            \"n_ctx\": MaxNewTokens,\n",
    "            \"max_tokens\": MaxNewTokens,\n",
    "            \"n_batch\": 512,  # set this based on your GPU & CPU RAM\n",
    "        }\n",
    "        if device_type.lower() == \"mps\":\n",
    "            kwargs[\"n_gpu_layers\"] = 1\n",
    "        if device_type.lower() == \"cuda\":\n",
    "            kwargs[\"n_gpu_layers\"] = 100  # set this based on your GPU\n",
    "\n",
    "        return LlamaCpp(**kwargs)\n",
    "    except:\n",
    "        if \"ggml\" in model_basename:\n",
    "            logging.INFO(\"If you were using GGML model, LLAMA-CPP Dropped Support, Use GGUF Instead\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750b2bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(device_type, model_id, model_basename=None, LOGGING=logging):\n",
    "\n",
    "    logging.info(f\"Loading Model: {model_id}, on: {device_type}\")\n",
    "    logging.info(\"This action can take a few minutes!\")\n",
    "\n",
    "    if model_basename is not None:\n",
    "        if \".gguf\" in model_basename.lower():\n",
    "            llm = load_quantized_model_gguf_ggml(model_id, model_basename, device_type, LOGGING)\n",
    "            #return llm\n",
    "\n",
    "    # # Load configuration from the model to avoid warnings\n",
    "    # generation_config = GenerationConfig.from_pretrained(model_id)\n",
    "    # # see here for details:\n",
    "    # # https://huggingface.co/docs/transformers/\n",
    "    # # main_classes/text_generation#transformers.GenerationConfig.from_pretrained.returns\n",
    "\n",
    "    # # Create a pipeline for text generation\n",
    "    # pipe = pipeline(\n",
    "    #     \"text-generation\",\n",
    "    #     model=model,\n",
    "    #     tokenizer=tokenizer,\n",
    "    #     max_length=MaxNewTokens,\n",
    "    #     temperature=0.2,\n",
    "    #     # top_p=0.95,\n",
    "    #     repetition_penalty=1.15,\n",
    "    #     generation_config=generation_config,\n",
    "    # )\n",
    "\n",
    "    # local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    # logging.info(\"Local LLM Loaded\")\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbf6a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"TheBloke/Llama-2-7b-Chat-GGUF\"\n",
    "MODEL_BASENAME = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "SOURCE_DIRECTORY = f\"/content/Data\"\n",
    "\n",
    "PERSIST_DIRECTORY = \"/content/DB\"\n",
    "def retrieval_qa_pipline(device_type, use_history, promptTemplate_type=\"llama\"):\n",
    "\n",
    "    embeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": device_type})\n",
    "    # uncomment the following line if you used HuggingFaceEmbeddings in the ingest.py\n",
    "    # embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "    # load the vectorstore\n",
    "    db = Chroma(\n",
    "        persist_directory=PERSIST_DIRECTORY,\n",
    "        embedding_function=embeddings,\n",
    "        client_settings=CHROMA_SETTINGS\n",
    "    )\n",
    "    retriever = db.as_retriever()\n",
    "\n",
    "    # get the prompt template and memory if set by the user.\n",
    "    prompt, memory = get_prompt_template(promptTemplate_type=promptTemplate_type, history=use_history)\n",
    "\n",
    "    # load the llm pipeline\n",
    "    llm = load_model(device_type, model_id=MODEL_ID, model_basename=MODEL_BASENAME, LOGGING=logging)\n",
    "\n",
    "    if use_history:\n",
    "        qa = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",  # try other chains types as well. refine, map_reduce, map_rerank\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,  # verbose=True,\n",
    "            callbacks=callback_manager,\n",
    "            chain_type_kwargs={\"prompt\": prompt, \"memory\": memory},\n",
    "        )\n",
    "    else:\n",
    "        qa = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",  # try other chains types as well. refine, map_reduce, map_rerank\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,  # verbose=True,\n",
    "            callbacks=callback_manager,\n",
    "            chain_type_kwargs={\n",
    "                \"prompt\": prompt,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    return qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ba0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "\n",
    "# get the prompt template and memory if set by the user.\n",
    "qa = retrieval_qa_pipline(use_history=True,device_type=\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0171e20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "        query = input(\"\\nEnter a query: \")\n",
    "        if query == \"exit\":\n",
    "            break\n",
    "        # Get the answer from the chain\n",
    "        res = qa(query)\n",
    "        answer, docs = res[\"result\"], res[\"source_documents\"]\n",
    "\n",
    "        # Print the result\n",
    "        print(\"\\n\\n> Question:\")\n",
    "        print(query)\n",
    "        print(\"\\n> Answer:\")\n",
    "        print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
